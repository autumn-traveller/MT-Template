%!TEX root = ../thesis.tex

%This is the Summary
%%=========================================
\cleardoublepage
\addcontentsline{toc}{section}{Abstract}
\section*{Abstract}

The growth of computational power and the increasing importance of digital data in scientific research has led to greater demand for computational resources  by users who aim to process large datasets. Particularly in the natural sciences it has become common for scientists to break down their computing needs into a sequence of smaller tasks, a so called workflow. These workflows can then be run on a variety of different execution platforms, depending on the users needs.

Since workflows are composed of segregated inter-dependent tasks which can run independently of each other, the individual tasks which make up a workflow can be assigned a fraction of the computational resources available to the entire execution platform, and doing so intelligently could improve efficiency and performance.

This thesis aims to investigate how reinforcement learning can be applied to the allocation of resources to individual tasks in order to choose more efficient allocations. A reinforcement learning solution will be integrated into the source code of a popular scientific workflow management system and tested with several common bioinformatic workflows. Two different approaches (Gradient Bandits and Q-learning) will be used and their performance will be judged alongside the performance of the task's default resource configurations and the resource allocation chosen by an approach using a feedback loop based on \cite{tovarjob,FeedbackBasedAllocation}.

Ultimately the approaches presented in this thesis outperform the workflow's default configurations with regards to both memory and CPU efficiency: the q-learning approach assigned 7\% less CPU hours, 31\% less memory and was 7\% faster, whereas the gradient bandit approach assigned 42\% less CPU hours, 80\% less memory and was only 4\% slower. The feedback loop approach assigned 13\% less CPU hours, 87\% less memory and was 7\% faster and thus performed worse than the gradient bandits with regards to CPU efficiency but better in terms of memory and speed. 

%%=========================================
%\cleardoublepage
\addcontentsline{toc}{section}{Zusammenfassung}
\section*{Zusammenfassung}

Mit der zunehmenden Wichtigkeit von digital Daten in den wissenschaftlichen Forschung und der stetig wachsenden Rechenleistung gibt es einen grösseren Bedarf an Rechenressourcen von Nutzer die grosse Datensätze analysieren müssen. Besonders in den Naturwissenschaften ist es üblich das Wissenschaftler ihre Rechenbedarfe in kleinere Sequenzen von "Tasks" aufteilen, so genannte "Workflows". Diese Workflows können dann auf verschiedene Plattformen ausgeführt werden.

Da diese Workflows aus verschiedene, eigenständige Tasks bestehen können sie unabhängig voneinander ausgeführt werden und müssen dabei nur einen Bruchteil der Rechenressourcen des ganzen Plattforms zugewiesen werden. Dies bietet die Möglichkeit an durch intelligente Allokationen die Effizienz und Leistung der Workflows zu verbessern. 

Diese Arbeit befasst sich mit der Forschungsfrage wie und mit welchem Effekt Reinforcement Learning für die Allokation von Rechenressourcen zu den Tasks eines Workflows angewandt werden kann. Zwei verschiedene Ansätze (Q-learning und Gradient Bandits) werden in den Quellencode einer Workflow Verwaltungssoftware integriert und mit fünf Bioinformatik Workflows versucht. Diese Ergebnisse werden verglichen mit der Leistung der Workflows mit ihre normale Bedingungen und einen Rückkopplungsschleifenansatz der auf \cite{tovarjob,FeedbackBasedAllocation} basiert.

Die im laufe dieser Arbeit entwickelten Ansätze zeigen eine bessere Leistung als die Workflows mit Voreingestellte Allokationen sowohl im Bezug auf die Speichereffizienz als auch auf der CPU Effizienz. Der q-learning Ansatz allozierte 7\% weniger CPU stunden, 31\% weniger speicher und war 7\% schneller. Der gradient bandit Ansatz allozierte 42\% weniger CPU stunden, 80\% weniger Speicher und war dafür nur 4\% langsamer. Der Ansatz mit der Rückkopplungsschleife allozierte 13\% weniger CPU stunden, 87\% weniger Speicher und war 7\% schneller, und war damit weniger Effizient mit seiner CPU Allokationen als die gradient bandits aber besser in Bezug auf Speicher und Geschwindigkeit.

%\newpage
%\addcontentsline{toc}{section}{Zusammenfassung}
%\section*{Zusammenfassung}

%Der Aufstieg des Internet of Things (IoT) stellt uns vor neue Herausforderungen bezueglich der Speicherung, Verarbeitung und Darstellung von Daten
