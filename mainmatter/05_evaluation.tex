%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Evaluation}
\label{cha:evaluation}

This chapter looks at the set-up for the experiments and the results achieved. First, in section \ref{sec:testing} the environment in which the agents were tested is explained and then in the section \ref{sec:analysis} the results are presented and discussed. 

\textbf{TODO: move this to conclusion section} Then section \ref{sec:_pot_improvements} considers what could be improved or done differently . Finally section \ref{sec:implications} looks forwards and asks what implications this could have and what the next steps would be.


\section{Testing the Agents}
\label{sec:testing}

In order to evaluate the performance of these agents they were tested on a 32-core, 125GB RAM machine against 5 different workflows from the popular bioinformatics framework nf-core \footnote{https://nf-co.re/}. Each workflow is run 10 times without any reinforcement learning agents active. This was done to have something to compare the results to later but also so that there was historical data about the average time each of the tasks take since the reinforcement learning agents need this information. After that the agents are tested. The gradient bandits are run 50 times for each workflow and the q-learning agents 100 times. As has been explained in \ref{subsub:states} the q-learning agent needs considerably longer to explore its extensive state-action space and as such is run for longer. However even though it is run twice as many times, the q-learning agent is still perhaps under-trained since the state-action space it needs to explore is so much larger than the action-space which the gradient bandits explore. 

Four different reinforcement learning agents are trained: the CPU bandit, a co-location of the CPU bandit and the memory bandit, the CPU only q-learning agent, and the CPU and memory q-learning approach. 

The five workflows used are the following: 1) \textit{eager} - a pipeline for genomic NGS sequencing data, 2) \textit{mhcquant} - a workflow for quantitative processing of data dependent (DDA) peptidomics data, 3) \textit{nanoseq} - which is an analysis pipeline for Nanopore DNA/RNA sequencing data, 4) \textit{viralrecon}- which can be  used to perform assembly and intra-host/low-frequency variant calling for viral sample, and 5)  \textit{metaboigniter} - a pipeline for pre-processing of mass spectrometry-based metabolomics data. 

The input data used for the workflows is based on custom combinations of different input files provided by the workflow's maintainers. Generally for each pipeline there are full example datasets and short example datasets as well as configurations for each of them. The short datasets are all configured to only assign either 1 or 2 CPUs and are thus are not really feasible for comparison, however the full datasets, whilst featuring realistic configurations, often take far too long to run. For reference, the full datasets will often take 6 hours or more to run once whereas the short examples never take more than 5 minutes. In order to build good pipelines with more realistic runtimes, configurations and input data, but without running too long, the short examples and the full ones need to be combined. The combinations are designed to strike a balance between having large input data and realistic task configurations but also being short enough that it is feasible to run the workflows hundreds of times. Each one is built differently but the overarching tactic used was to start with the command line arguments to the pipeline from the short examples and combine it with the configuration and the input data from the full dataset. After this the list of input files is reduced until the pipeline completes in a reasonable amount of time. Ultimately, by following this process, configurations and inputs were found for all five of the pipelines mentioned above such that they were completing after around 10 to 40 minutes.  

The nextflow source code inherently keeps track of the total number of resources available and the resources already assigned to tasks and it will not schedule more resources than the system has. The tasks are all run inside of docker containers with the exact amount of CPUs and memory requested by the task. Since the tasks are all being run on the same machine it would have been possible to ignore the sum of the resource requests and over-assign resources so that all of the tasks compete with each other, but this does not reflect situations where pipelines are being run on clusters with resource managers, and as such this was left unchanged.

For the final comparison in the evaluation chapter the 10 runs with the default configurations are compared to the last 10 runs with each of the agents.



%%=========================================
\section{Analysis of Results}
\label{sec:analysis}

\subsection{Performance Comparison}
\label{sub:comp_perf}

\begin{figure}
    \centering
        \includegraphics[width=\textwidth]{fig/cpu_mem_results.png}
        \caption{Performance of the CPU and Memory Approaches}
        \label{fig:cpu_results}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=\textwidth]{fig/cropped_memory_usage_final.png}
        \caption{Memory Usage}
        \label{fig:mem_use}
\end{figure}

In this section the performance of the gradient bandit and the q-learning approaches over their last 10 runs will be compared with each other and with the performance of 10 runs of the same workflows with their default configuration.

The figures \ref{fig:cpu_results} and \ref{mem_use} show the performance of the 3 different approaches tried as well as the performance of the default configuration. The left graph in the first figure shows the total amount of CPU hours requested by all of the tasks across all of the workflows and the amount of CPU hours that were effectively used (calculated as $cpu\_usage/100*time$ and $cpus*time$ respectively). The right graph in this figure shows the total time it took for all of the workflows to complete. Additionally in \ref{fig:mem_use}, the memory usage is shown. The unit measured in the right graph is the gigabyte hour and the graph displays the total amount of gigabyte hours allocated across all of the workflows as well as the actual amount of gigabyte hours used (based on each task’s peak RSS and execution time- this is the minimum amount of gigabyte hours which could have been assigned without the task being killed). The left graph shows the average memory usage of each individual task (calculated as $\frac{peak\_rss}{memory}$). 

From these graphics it is clear to see that all of the reinforcement learning approaches managed to improve CPU efficiency and allocate less CPU Hours than both the default configuration and the naive feedback loop approach. In the case of the gradient bandits it was particularly effective at keeping the overall CPU hours assigned to an absolute minimum, however it seems that this efficiency came at the cost of speed and it performed worse than its q-learning counterpart in this aspect and took slightly longer than the default configuration. The q-learning approach took almost exactly the same amount of time as the feedback loop but was more efficient with regards to CPU usage. As can be seen, both of the reinforcement learning approaches are able to assign less CPU hours without having a detrimental effect on execution time. Looking at the reward function \ref{q_agent_1_reward} for the q-learning agent the “CPU usage”considered by the function is not the task’s usage of the task but rather $max(90,CPU\_usage)$. As such agents for tasks which exhibit very high CPU usage ($>90\%$) receive no additional reward for increased efficiency (since efficiency can always be increased by decreasing the total number of CPUs assigned) so their rewards depend entirely on the execution time and they are thus encouraged to find resource allocations which decrease runtime. This most likely explains why the gradient bandit is more efficient but the q-learning agent is faster. Interestingly, the feedback loop ultimately wound up increasing the effective CPU hours used by the workflows, something which had remained constant under the other 3 configurations, and was fairly efficient in terms of the total amount of CPU hours assigned relative to the increased effective usage. It can only be speculated as to why it used more actual CPU hours than the other approaches but it may be down to the fact that certain highly parallelizable tasks will be assigned incredibly large numbers of CPUs (e.g. a task with 2800\% CPU usage during training will be assigned 28 CPUs), far beyond the point of diminishing returns, and eventually lead to the workflow using more effective CPU hours. This would indicate that assigning more resources may increase the amount actually used (something which the feedback loop approach assumes is false). Finally, regarding execution time, it is interesting to see that the q-learning approach is able to match the performance of the feedback loop despite assigning less CPU hours overall. In this regard the q-learning approach could be considered optimal relative to the other approaches. The gradient bandit ultimately over-emphasized decreasing costs and performed worse whilst the feedback loop approach over-emphasized attempting to increase performance by increasing CPU usage and wound up using more resources than would have been necessary to achieve the same execution time. The q-learning agent was able to find the best balance between resource usage and performance. This is of course the benefit of a reinforcement learning approach- it can estimate the values of a complex reward function by exploring different actions and then learn to pick the maximizing action. The feedback loop approach cannot explore different allocations on its own and is dependent on the correctness of its assumptions and the quality and scope of its input data.

Concerning memory allocations, both of the reinforcement learning approaches proved to be better than the default configuration with regards to this resource. One aspect which stands out is how poor the default configurations are. Indeed on average they are using just 5\% of the assigned memory on a task by task basis . On the other hand the q-learning approach is not much more efficient however it still reduces the overall amount of wasted memory by a large amount (~5000 gigabyte hours). Yet this amount is dwarfed by the other two approaches which save close to 15000 gigabyte hours. The memory bandit approach performed much better than the q-learning approach by a significant margin and it assigned less than half as much as either the default configuration or the q-learning agent. The feedback loop approach is clearly the best, assigning the least amount of memory hours overall and showing very high memory usage for all of the tasks. Ultimately the performance of the q-learning agent is down to the fact that the number of chunks of memory it could assign was very low, in order to have less states so it could be trained faster, and thus it had a fairly limited extent to which it could reduce the memory usage. The gradient bandit suffers from this too because ultimately it also only had a fixed number of memory chunks to choose from and thus could only increase or decrease a task’s memory usage by the size of the chunk. The feedback loop does not have this problem and can assign memory at the most granular level possible. It therefore follows that it would naturally be able to outperform these two approaches which can only assign “discrete” chunks of memory.


\subsection{Summary of Results}
\label{sub:summary}

In conclusion all of the reinforcement learning approaches were able to intelligently size the tasks in the workflows and in the end the overall number of CPU hours which were wasted was reduced (even relative to the feedback loop). In addition to this the agents were also able to significantly reduce the memory usage compared to the default configuration but struggled to reach the near-optimal performance of the feedback loop. The gradient bandits took slightly longer on average but were much more efficient for that, whilst the q-learning agent struck a better balance between resource efficiency and increased performance and the q-learning approach for both CPU and memory was ultimately able to outperform the default configurations in both resource efficiency and runtime.

While it could be argued that the q-learning agent is therefore the “better” options it is hard to say exactly what the right approach is because it also depends on the desires of the user. Many scientists may be prepared to accept slightly longer runtimes if the “greediness” of their workflows is decreased and they are able to free up resources for others to use, or reduce their own costs (if they are paying for CPU hours). Indeed from a financial perspective the gradient bandit wastes significantly less CPU hours than the extra time it takes to complete (which is fairly marginal). However in a situation where the user is paying for the entire system (and not resource usage) it may make more sense from a financial perspective to use the q-learning agent approach since it reduces the amount of time for which the system needs to be up since the overall execution time is reduced. Finally, it should be pointed out that combining the feedback loop approach and a q-learning agent which only considers CPU assignments would yield the best overall performance.