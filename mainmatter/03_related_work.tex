%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Related Work}
\label{cha:related_work}

This chapter discusses other works in related areas and considers the nuances of their approaches and how they differ from the one presented in this thesis. First there will be a discussion of select pieces of literature which use reinforcement learning approaches to address similar problems. Then three papers related specifically to resource management within the context of scientific workflows will be considered.

%%=========================================

\section{Using Reinforcement Learning to Allocate Resources}
 \label{sec:rl_allocation}
 
SmartYARN (\cite{smartYarn}) applies a q-learning approach to balance resource usage optimization against application performance. This is one of the central considerations for any client of a cloud computing platform- the need to balance reducing costs by using less resources against the need to increase runtime by using more resources. In this paper the researchers used the performance of the application under a certain resource configuration as the state-space, while the action-space was made up of just 5 actions: increasing or decreasing one unit of CPU or memory or keeping the previous allocation. In the end the researchers found that the agent was able to achieve 98\% of the optimal cost reduction and generally performed at the optimal level, finding the optimal allocation the vast majority of the time.

This paper served as the inspiration for the q-learning approach used in this paper and is also one of the few works discussed in this section which considers the central issue of the cost versus performance when assigning resources. One significant difference however is that SmartYARN only seeks to optimise individual jobs which do not depend on each other whereas for scientific workflows the jobs (or tasks) are part of a larger workflow and depend on each other and may need to wait for one another.

\cite{vconf} tackled a similar problem- configuring the resource allocations of virtual machine's (VM's) using reinforcement learning. Their approach was a continuing discounted MDP with q-learning. The states were always a triple of CPU Time credits (used for scheduling cpu time), virtual cpu's (vCPU's), and memory. The agent's available actions were increasing, decreasing or leaving the allocations, with only one resource allowed to be changed per action. As a reward the ratio of the achieved throughput to a reference throughput was used. The key trick used by the researchers was to use model based reinforcement learning. By modelling states the agent is able to simulate or predict the reward it can expect from previously unseen action-state pairs, whereas the classic q-learning approach (also used in this paper) requires the agent to experiment with each action-state pair. This means the model based agent is able to learn much faster and can enter its exploitation phase earlier than the static agent which must explore the action-state space for a long time, especially when there are large state spaces. Ultimately however the researchers found that the static agent performed quite well and achieved high levels of throughput in its own regard, although the model-based approach consistently outperformed the static one. 

This paper's problem and its solution to it are again quite similar to those of this thesis but there is of course a significant difference since it schedules the resources of virtual machines as opposed to the resources of individual jobs. Another difference is of course the use of model based reinforcement learning to limit the time spent exploring the state action space for optimal rewards and spend more time exploiting this to achieve better results. 

Finally there is the DeepRM \cite{DeepRM} paper on resource management using deep reinforcement learning. Here the researchers used a policy gradient reinforcement learning algorithm combined with a neural network. The state-space consisted of the resources of the cluster and the resource requirements of arriving jobs, encoded as so called "images" which could be fed into the neural network. The actions available to the agent were simply to choose a job and allocate resources to it. In order to speed up the process the agent was given the option to schedule up to $M$ jobs in a row, thus enabling it to leap forward by as many as $M$ timesteps instead of always progressing by a single timestep. The reward given to the agent was the negative average slowdown. In the end the researchers' approach outperformed both the shortest job first metric and a very strong, heuristic based algorithm called Tetris \cite{tetris}. A key reason that the agent was able to achieve this increased performance was that it learned a policy to maximize the number of small jobs it completed. While the approach taken in the paper was still better with large jobs it was significantly better with small jobs because the network learned to always keep a small amount of resources available to quickly service any small jobs which might arrive, whereas the other approaches inevitably scheduled all the available resources so that small jobs also had to wait.

The similarities here are relatively few since this paper also had to concern itself with the decision of which job to allocate and when, whereas this thesis only considered how much of a given resource to allocate to a task. Beyond that there is also the fact that a neural network was employed and the decision to use the resources of the system to model the state of the agent. Though this is an idea which could be interesting as a topic of further research if one were to expand on this thesis. What is most interesting however is the policy that the neural network was able to learn. It was able to maximise throughput by always reserving a small amount of resources for small tasks which re-occur often. This kind of advanced strategy is something that could of course be built into an algorithm but the ability of a reinforcement learning agent to learn such a policy shows their potential for impressive results within the field of task scheduling.

\section{Resource Allocations in Scientific Workflows}
\label{sec:workflow_allocation_papers}

 In \cite{tarema} the researchers used cluster profiling to build node groups with similar performance profiles and map a task to a node group based on historical performance data. A 3 step system is used. In step 1 the cluster profiles are built based on microbenchmarks and hardware analysis tools. Then in step 2 the tasks are labelled in the SWMS based on their monitoring data. Finally, in the third step dynamic task-resource allocation is performed based on the information from the previous steps. Using this approach the authors were able to achieve a mean reduction of job runtimes of 19.8\% compared to popular schedulers, and a reduction of 4.54\% compared to the shortest-job-fastest-node (SJFN) heuristic. 

This target of that paper is of course much more similar to that this thesis than any of the previously mentioned papers however the obvious difference is that this thesis approaches task resource assignment with a reinforcement learning approach. One other similarity is that this paper used the exact same SWMS (nextflow) as this thesis. Beyond that it would be an interesting area of research to try and combine the topic of that paper with the topic of this thesis and try and apply reinforcement learning to task resource assignment amongst node groups with similar performance profiles.

Next, in \cite{FeedbackBasedAllocation} a feedback based resource allocation system for the scheduling of scientific workflows is designed. Their approach uses an online feedback-loop to improve resource usage predictions and thus more accurately allocate resources to the constituent tasks of a scientific workflow. Tasks within scientific workflows were assigned memory based on percentile or linear regression predictors. One such percentile predictor, called PC50, worked by using the median memory usage of a task as its prediction. This approach was significantly outperformed by the \textit{LR mean} approach which was a linear regression predictor that predicted memory usage based on a linear regression that relates a task’s input data to its memory usage. This prediction is then offset by the standard deviation between the predicted and the true peak memory usage.

This paper addresses a very similar problem to that of this thesis but attempts to use linear regression to predict memory usage whereas this paper is focused on using reinforcement learning. It also of course does not attempt to predict CPU usage or assigned CPUs to tasks. The PC50 approach and \textit{LR mean} approaches ultimately served as inspiration for the naive approach to which the performance of the agents in this thesis are compared. The naive approach used here essentially takes the PC50 predictor and adds an offset based on standard deviation just like \textit{LR mean}. 

Finally there is \cite{tovarjob} which presents a job sizing strategy for tasks in scientific workflows. The researchers used equations based on integrals over the resource usage to either minimise waste or maximise throughput. When gather training data they assigned tasks the maximum allowable value for all of their resources. This was repeated until the task’s resource usage statistics began to converge. Then they used their analytical solution to assign resources and if the task failed or was killed it was retried again with the maximum available resources. Ultimately an increase in throughput of between 10 and 400 was achieved.

This idea also served as inspiration for the naive approach used here, namely the idea to assign the maximum resources available and observe a task’s performance is an interesting strategy for determining how to size a task. This of course assumes that a task’s resource usage is independent of its available resources. It is also a notably different approach to exploring the available space of resource allocations from that of the reinforcement learning approaches used here. Lastly unlike in \cite{FeedbackBasedAllocation}, \cite{tovarjob} also looks at CPUs and how to assign this resource to tasks within scientific workflows.

\section{Summary of Similarities and Points of Interest in these Works}
\label{sec:takeaways}

All of the papers discussed in section \ref{sec:rl_allocation} managed to utilize reinforcement learning in an administrative fashion to improve the performance of jobs in their respective computing environments. A common theme among the papers concerned with resource utilization was to limit the action space to increasing, decreasing or keeping the current allocation or resources and storing the current allocation as the agent’s state. The majority of the papers also utilized neural networks and it is also worth noting that most of them used runtime in their reward functions. A key difference between all of the above papers and this thesis is that these papers handled the direct allocation of resources among the machines in their environment to tasks or jobs, whereas in a scientific workflow management system the tasks are given a resource allocation by the SWMS and this is then passed on to a platform or cluster management software which handles the actual allocation of the job to a machine. Indeed it is possible for these two learning systems to interact with one another without knowing it.

In section \ref{sec:workflow_allocation_papers} the topic of resource management in scientific workflows was looked at and the papers mentioned all tended to use more analytical approaches. Indeed there is relatively limited literature regarding combining machine learning and scientific workflow management. A consistent theme in the papers about sizing tasks in scientific workflows is the idea that a task’s resource requirements are relatively static and can be learned over time using linear regression or profiling.  One topic which is not really addressed in these papers is the relationship between resource assignment and performance, because assigning a task more resources (which may be less efficient) can still speed up performance. This is an important topic in this thesis and the papers \cite{FeedbackBasedAllocation} and \cite{tovarjob} do not mention runtime, while \cite{tarema} does not deal with task sizing. In this thesis the effect of task sizing on workflow performance is an delicate tradeoff which must be balanced correctly to both reduce resource wastage and maintain reasonable runtimes and improve (or at least not harm) workflow performance.

