%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Introduction}
\label{cha:introduction}

%%=========================================
\section{Motivation and Problem Description}
\label{sec:motivation}

The rising popularity of scientific workflows and the enormity of their constituent task's aggregate resource requirements \cite{ResourceProvisioning} presents an opportunity to try and fine tune the resources allocated to these tasks to achieve better performance.

Since a scientific workflow is composed of many tasks which each have unique relationships between the resources they require, their input, and how it affects their performance, it becomes cumbersome or even futile to try and hand-pick the most efficient resource allocation for each individual task. In addition to this it is also quite common that the users may not have the requisite knowledge to properly size the tasks themselves and may make poor estimations \cite{Predictability} of what a given task's resource requirements are. When sizing tasks one also needs to find a balance between resource efficiency and improved execution time. Assigning a task more resources will usually make it execute faster but may increase costs. Additionally in an environment with limited resources assigning a task too many resources hampers other tasks and decreases the resources available to them. Finally it must also be considered that for a given use case the exact topology of the deployment scenario (i.e. whether it is being run on an individual computer, in a grid or cluster, or on a cloud computing platform) and the hardware infrastructure being used will also influence the performance of the workflow and its constituent tasks.  When one considers all of these factors it becomes clear that fine tuning the resources for each of the tasks within such workflows "by hand" is difficult and complex. 

However it is also obvious that sizing tasks properly is of critical importance. Specifically for large scale, distributed computations the resources being requested are so large that even small improvements in efficiency represent a large amount of reclaimed resources which could be allocated to other users or could reduce costs (in a scenario where computational resources are being paid for by the workflow user). 

Thus this presents a scenario in which although better task sizing is beneficial for everyone it is not a problem with a simple solution.

At this point it becomes quite reasonable to begin to consider whether machine learning could provide a solution. There are many different methods which could be tried, however reinforcement learning presents two distinct advantages. Firstly it does not need to be trained with data from good allocations in the past because reinforcement learning agents 'learn on the go' by trying to discover the optimal policy for their goal through interaction with their environment. This is important because the problem of verifying a given resource allocation is optimal is exactly as hard as finding an optimal allocation, so gathering training data with examples of optimal allocations is ruled out. Secondly, reinforcement learning is adaptable. Because it only ever aims to learn an optimal policy though interactions, a reinforcement learning agent is constantly gathering feedback on its actions (even after it has reached a point where it could be considered 'optimal') and constantly trying to refine its approach to be as perfect as possible. Therefore if the environment or the problem changes and the old policy is no longer the best one, the agent will notice this and adjust its approach and learn a new policy to achieve its goal. Within the context of scientific workflows this is desirable because the tasks and the profile of the input data may change (over time or at once) and render the previous allocations of resources for the given task completely wrong. Indeed the entire system to which the workflow is deployed could change, for example migrating to new infrastructure or a different cloud computing provider. In all of these scenarios the way that resources are allocated to tasks would probably need to be adjusted and reinforcement learning is an approach which would be able to handle these changes and adapt to them.

Ultimately reinforcement learning presents an advantage for task sizing because it can handle the complex job of exploring different resource allocations and attempting to balance resource efficiency against faster execution. This is something which approaches such as linear regression or picking the average historical usage may struggle to do. The great advantage of reinforcement learning should be its ability to explore different allocations on its own and leverage them to increase performance.

%\LTXtable{\textwidth}{tab/scenario1_sensor}

%%=========================================
\section{Goal of the Thesis}
\label{sec:goal}

The goal of this thesis is to use reinforcement learning to improve the efficiency and performance of scientific workflows by more accurately assigning resources to the individual tasks within the workflows. Specifically the CPU and memory usage will be examined using two different approaches: Gradient Bandits and Q-learning, and compared to the performance of the task's default configuration and a feedback loop approach which combines different aspects of the methods used in \cite{tovarjob,FeedbackBasedAllocation}.

%%=========================================
\section{Structure of the Thesis}
\label{sec:structure}

The remainder of the thesis is structured as follows: in chapter \ref{cha:background} an overview and an explanation of the ideas and technologies relevant to this paper is given. Then in related work (chapter \ref{cha:related_work}) some papers are discussed which addressed similar topics and served as influences for this thesis. After that the implementation of a solution to the problem is described in the approach (chapter \ref{cha:approach}) and finally in chapter \ref{cha:evaluation} the results are presented and analysed. The thesis concludes in chapter \ref{cha:conclusion} with a brief summary, a review of potential improvements and a look towards future research.
