%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Related Work}
\label{cha:related_work}

This chapter discusses other works in related areas and discusses the nuances of their approaches and results. 

%%=========================================
\section{Reinforcement Learning to Schedule Tasks/Jobs}
\label{sec:rl_scheduling}

In the LearningToSchedule... paper the researchers attempted to use reinforcement learning to schedule data-parallel processing jobs in shared clusters. In particular they considered resource management systems such as YARN or Mesos. Their approach was to schedule the jobs based on co-location goodness, which is a metric for how well a job can share the resources of the machine which it is scheduled on with another job. This metric was based on cpu usage and disk and network IO since these two activities tend to be very compatible, while the job waits on the disk or network another job can be free to use the cpu and vice versa. The paper considered three different scheduling approaches: FIFO, a gradient bandit which starts new each time and a gradient bandit which is starts with the data from previous runs. In the end the best performing approach was the bandit which had the benefit of the data from previous runs. This was because that bandit skipped the exploration phase during which bandit sacrifice performance for learning by virtue of having the previous runs' data. This gradient bandit approach was then incorporated into three different schedulers called Mary, Hugo and Hugo*.  Mary schedules based on co-location goodness, Hugo extends this by aggregating jobs into groups based on their similarities and scheduling the groups, and Hugo* does the same as Hugo but also considers how long a task has been waiting. 

In  the SCARL paper researchers use reinforcement learning for scheduling jobs among machines with heterogeneous resources. Their approach was based on combining attentive representation and the policy-gradient method. Attentive representation is a technique for focusing attention more quickly within a neural network. The model which they used to represent the problem used the allocation of jobs to machines as the state whilst the available actions at any given point were the scheduling of jobs. The reward function was based on a slowdown metric: $(elapsed\_time * penalty\_ factor) / computation\_time$. Ultimately the researchers found that for high levels of heterogeneity SCARL outperformed the shortest job first (SJF) metric by 10\% and for lower levels of heterogeneity it outperformed it by 4 \%.

\section{Reinforcement Learning to Allocate Resources}
 \label{rl_allocation}
 
SmartYARN applied a Q-learning approach to balance resource  usage optimization against application performance. This is one of the central considerations for any client of a cloud computing platform- the need to balance minimizing cost by using less resources against the need to increase runtime by using more resources. In this paper the researchers used the performance of the app under a certain resource configuration as the state-space and the actions available to the agent were increasing or decreasing one unit of cpu or memory or keeping the previous allocation. In the end the researchers found that the agent was able to achieve 98\% of the optimal cost reduction and generally performed at the optimal level, finding the optimal allocation the vast majority of the time.

VMConf tackled a similar problem- configuring the resource allocations of virtual machine's (VM's) using reinforcement learning. Their approach was a continuing discounted MDP with Q-learning. The state space was a triple of CPU Time credits (used for scheduling cpu time), virtual cpu's (vCPU's), and memory. The agent's available actions were increasing, decreasing or leaving the allocations, with only one resource allowed to be changed per action. As a reward the ratio of the achieved throughput to a reference throughput was used. One key trick used by the researchers was to use model based reinforcement learning. By modeling states the agent is able to simulate or predict the reward it can expect from previously unseen action-state pairs, whereas the classic approach requires the agent to experiment with each action-state pair. This means the model based agent is able to learn much faster and can enter its exploitation phase earlier than the static agent which must explore the action-state space for a long time when there are large state spaces. Ultimately however the researchers found that the static agent performed quite well and achieve high levels of throughput in its own regard, although the model-based approach consistently outperformed the static one. 

Finally there is the DeepRM paper on resource management using deep reinforcement learning. These researchers used a policy gradient reinforcement learning algorithm combined with a neural network. The state-space consisted of the resources of the cluster and the resource requirements of arriving jobs, represented as images which could be fed into the neural network. The available actions were simply to allocate the jobs. In order to speed up the process the agent was given the option to schedule up to $M$ jobs in a row, thus enabling it to complete leap forward by as many as $M$ timesteps instead of always progressing by a single timestep. The reward given to the agent was the negative average slowdown. In the end the shortest job first metric and a very strong, heuristic based algorithm called Tetris were both outperformed by the researchers' approach. A key takeaway was that the agent achieved this increased performance by learning a policy to maximize the number of small jobs it completed. While the approach taken in the paper was still better with large jobs it was distinctly better with small jobs because the network learned to always keep a small amount of resources available to quickly service any small jobs which might arrive, whereas the other approaches inevitably scheduled all the available resources so that small jobs also had to wait.

\section{Takeaways}


All of the papers discussed above managed to utilize reinforcement learning in an administrative fashion to improve the performance of jobs in their respective computing environments. A common theme among the papers concerned with resource utilization was to limit the action space to increasing, decreasing or keeping the current allocation or resources. The majority of the papers also utilized neural networks or deep reinforcement learning. It is also worth noting that most of these papers used runtime in their reward functions. A key difference between all of the above papers and this thesis is that these papers handled the allocation of resources among machines in their system whereas in nextflow the jobs are given a resource allocation and then passed on to a platform or cluster management software which handles the actual allocation of the job to a machine. 
















































 
%\TODO{citations!!}  %\cite{TheInquisitiveMind}.

