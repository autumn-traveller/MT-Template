%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Conclusion and Outlook}
\label{cha:conclusion}

This thesis has looked at the use of reinforcement learning to size tasks in scientific workflows. It has provided background information on scientific workflows, scientific workflow managers, the gradient bandit problem, and q-learning in chapter \ref{cha:background}. It has discussed related work in chapter \ref{cha:related_work} and then incorporated gradient bandits and q-learning into a popular workflow manager in chapter \ref{cha:approach}. The performance of this approach was then tested against that of the default configurations and that of a feedback-loop based approach. In the evaluation this thesis found that the results had vastly improved on the default configuration and performed similarly with the feedback loop approach for runtime performance whilst achieving greater CPU efficiency but worse memory efficiency.

This rest of this chapter briefly looks at what could be improved upon in section \ref{sec:improvements} before section \ref{sec:implications} looks forwards and asks what implications this could have and what avenues might prove interesting for further research.

%%=========================================
\section{Improvements}
\label{sec:improvements}

Immediately the most obvious way in which these approaches could be improved upon would be to incorporate read and write calls into the reward function. When a thread reads or writes characters from or to a device it must perform a system call and wait for it to complete, and during this time the thread cannot use the CPU. If a program has a high degree of parallel operations but also has to read and write lots of characters then it would probably have lower CPU usage (because of the system calls) but simultaneously would benefit from being assigned more CPUs. However because of the nature of the bandit and the agent’s reward functions it is unlikely that this task would be assigned more resources since read and write calls are not considered.

Moving along this train of thought, it could also be prudent to incorporate the size of a task’s input file into the reward function for the memory bandit or the q-learning agent. A task’s memory footprint is most likely quite closely coupled with the size of the input file and this could prove important information for a reinforcement learning agent.

A different issue which was relevant to both approaches was the length of time needed to train the agents. The gradient bandit was trained for about 40 runs and the q-learning agent took 90 runs before they were “tested” across the next 10 runs. For large scale workflows this would take a considerable amount of time. Of course the agents are able to learn on the go and the performance during the training time is not so much worse as to make the workflows unusable during that time, so it is hard to say how much of a problem this could be.

Specifically for the q-learning agent there is room for improvement by incorporating replay buffers and eligibility traces. These would enable the agent to update its value function for the states and actions which led it to its current state and this is could help to learn an optimal $Q$ function faster. This is not done in the current implementation and could have a significant impact on performance. Additionally an epsilon of $\epsilon = 1/t$ could be used to decrease the agent’s exploration proportional to the time. Additionally it may be more prudent to separate the memory assignments from the CPU assignments and instead co-locate a q-learning agent for CPU with one for memory. This is what was done for the gradient bandit and it would make sense for the q-learning agent too, the relationship between CPU and memory assignments can be split up as they are not hugely dependent on each other, and because it could then explore a larger state-action-space, and thus be trained faster or achieve better results by having more fine-grained allocation options.

Finally, considering the testing set-up it would be an improvement to use kubernetes clusters with different machines to more accurately simulate the scenario where a workflow is performed on a larger distributed system with different nodes (with potentially heterogeneous resources). Such a set-up may also change results as the cumulative resources of the system would be greater, which may allows the agent’s decreased resource usage to increase throughput even more and thus speed up the workflows to a greater degree. It would also be better to use random inputs chosen from several different sets of possible inputs as opposed to repeating the workflows with the same input data. By training the agents using the same inputs there is a danger of overfitting the agents’ resource assignments. This may be less of a danger for the CPU assignments, because the character of a task’s CPU usage is less dependent on the size of the inputs, but repeatedly using the same inputs for an agent which sizes tasks for memory does present a real danger of overfitting. This is because the memory footprint of a task is much more dependent on the input to the task. This recommendation is not particularly relevant for those workflows which tend to have similarly sized inputs, but it could make a difference for workflows with different usage patterns.

%%=========================================
\section{Implications and Further Areas of Research}
\label{sec:implications}

The obvious implications of these findings are that, much like some of the scheduling problems mentioned in chapter \ref{cha:related_work}, this is also an area in which reinforcement learning has interesting potential. Additionally it has also shown that the resource configurations in scientific workflows have room for improvement and are another area which should be investigated further.

Nextflow and other scientific workflow managers represent a different class of actor in the interactions between user, application and execution platform. These workflow managers do not manage the execution platform, nor do they create the workflows which are executed. They are a kind of middle man and thus present a unique perspective on user-executor interactions. Normally the interactions in such environments are just between the user and the execution platform. The user has control over what process it wants to perform and how many resources to ask for, while the execution platform has control over when and how to execute the process. However in the context of a scientific workflow and a workflow manager, the manager sits in between the user and the execution platform and has no control over the tasks that must be executed nor over the execution. It is only capable of scheduling and sizing the tasks. This position requires a slightly different approach to common problems such as scheduling and resource assignment but it also presents an interesting avenue to research novel approaches to these familiar problems.

Now of the resources most important to both the process being executed and the user, the disk space must rank as one of the most important. Although it was not covered in this thesis, sizing a task’s disk space is another area where intelligent resource management could prove useful. It could most likely be solved in a fashion similar to the approach used for memory and in fact it would be quite simple to co-locate a gradient bandit approach for disk space alongside the memory and CPU bandits. Incorporating this into a q-learning agent would be trickier since the agent already has to explore a large state-action space for just memory and CPU, and by adding disk space into this it could become a very long process to try and train such an agent. Ultimately it may prove better to have separate q-learning agents for the CPU and the memory/disk-space (since these two are more likely to be closely related).

If one considers the current trends in machine learning and reinforcement learning agents then what could also be an interesting avenue for research would be to use deep reinforcement-learning \cite{deepQ}, which uses a deep neural network to learn a policy. This could allow for the kind of fine-grained memory assignments that made the feedback loop perform so well.

Following up on this idea, one more interesting area for further research would be incorporating a machine learning approach into a scientific workflow management systems’ internal scheduler. Whilst the reinforcement learning agents used in this thesis had no knowledge of the other tasks and of their interdependencies in the workflow, a scheduler would have the full understanding of a workflow’s digraph and could perhaps improve performance to an even greater degree by intelligently choosing when to assign a task slightly more or less resources than usual, depending on which tasks must wait for the current one to complete. 

