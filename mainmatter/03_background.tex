%!TEX root = ../thesis.tex

\cleardoublepage
\chapter{Background}
\label{cha:background}


This chapter provides background information on and explanations of the functionality and purpose of some of the technologies and concepts related to this thesis.  To begin, the history of distributed computing and containerization are discussed in sections \ref{sec:dist-computing} and \ref{sec:containers} before moving on to a discussion of their current usage in scientific workflows in \ref{sec:workflows}. After that the workflow manager used in this thesis, nextflow, is introduced in section \ref{sec:nextflow}. Finally, in \ref{sec:rl} reinforcement learning is touched upon and its functionality as well as the benefits it could bring to the problem at hand are briefly discussed.

%%=========================================
\section{Distributed Systems, Batch Processing and Cloud Computing}
\label{sec:dist-computing}

Within the context of this thesis and scientific workflows it makes sense to discuss some of the ways in which tasks that have long run times and require large amounts of resources can be carried out. In general for a given user who needs to execute a workflow there are really only four options namely 1) running it on their own equipment, which quickly becomes infeasible for large and difficult tasks, 2) using a cloud computing service, 3) using a dedicated machine with more resources or 4) using a dedicated distributed system such as a cluster or grid. Of these four options number 1 and 3 obviously need no special explanation but cloud computing and distributed systems deserve to be briefly defined.

While there is a lot of debate to be had about what precisely constitutes a distributed system the paper by [TANNENBAUM et.al] defines a distributed system as one in which “multiple autonomous processors do not share memory but cooperate by sending messages over a communications network”. Building on this definition, for this thesis the term distributed computing is considered to be the performance of a single computational task across multiple distinct machines. Usually this task is quite large or difficult, or is an aggregate of various smaller tasks but the core idea remains the same. Within the term ‘distributed computational system’ many distinctions can be made regarding the exact architecture of the distributed system but the two which are specifically relevant to this thesis are only grids and the types of clusters typically used at universities.  One of the possible uses for distributed systems is for parallel high-performance applications because the vast resources available in the system.

When speaking of distributed systems two prominent architectures are grids and clusters. 

TODO: batch processing?

In this thesis a cloud computing service is considered a service with an aim to be available all the time and from anywhere, and to provide configurable computing resources to support the needs of the user. The critical component of that description is the word configurable. Cloud computing provides a flexible amount of computing resources so that users only pay for what they need. The most common resources in demand are data storage and computational power. With the continuing growth of the internet and digitalization and the increasing demand for computational power, cloud computing services continue to grow in popularity. Since the resources provided by such services can also located in almost any region they provide a great degree of power and flexibility for users and also liberate from having to acquire such resources themselves. A given user or group of users no longer need to own and manage their own systems of computing resources but can instead purchase them from cloud computing providers which relieves them of the burden of maintaining and managing such systems as well. 


%%=========================================
\section{Containerization}
\label{sec:containers}

Virtualization has existed for a long time however the overhead of running a virtual machine is often not worth the advantages it provides. Particularly for reproducible software development the most important qualities of the host machine may not even be the architecture but rather the interactions with the operating system and its filesystem. Software containers are a type of virtualisation which do not virtualise the architecture of the host machine but instead use namespaces and control groups to virtualise the operating system, network, filesystem and all of the other peripheral components a program interacts with. This enables containers to isolate processes from the host machine and also from each other. Specifically for the deployment of applications and software this provides a humongous advantage. With containers it is possible, for example, to run two different versions of the same software on the same machine or even to run two versions of the same software but with two different configurations in parallel to each other. In addition to this the software will not know anything of its other version and nor will any additional configuration be necessary in order to enable them to run on the same host. Most importantly, in both of these cases the behaviour of the programs in the containers should always be the same. This is because a software container provides a sanitized version of the host system where there is no danger of other users or processes interacting with the filesystem or using the devices made available by the operating system.

Beyond the benefits for co-locating services on the same machine without any danger of interference containerization also simplifies the process of deploying software to a new machine. If a given machine supports the running of containers then all that is needed to deploy one's software on that machine is the container. The poster child for containerization is Docker. Their philosophy is ``build once, deploy anywhere'', and many cloud computing services only need to be provided a Dockerfile (the configuration) and they can instantly deploy that service. This simplifies software deployment for the software developers as well as the management of the machines on which they run. In addition to this it also becomes easy to scale applications up or down. Adding more resources to a container or starting a new machine running the same application becomes a trivial process. Lastly, for managers of cloud computing centers the additional layer of virtualization offered by containers enables them to move containers and applications between different machines. 

This becomes relevant for scientific workflows because by using containers a workflow can be reliably reproduced on various different execution platforms. 

%%=========================================
\section{Scientific Workflows}
\label{sec:workflows}

%%=========================================
\section{Workflow Management Systems}
\label{sec:management}

%%=========================================
\section{Nextflow}
\label{sec:nextflow}

%%=========================================
\section{Reinforcement Learning}
\label{sec:rl}

Popularized in the seminal book (TODO: cite!) by Sutton and Barto, reinforcement learning presents a framework for an agent to learn the optimal policy for achieving a given goal in an uncertain environment by interacting with the problem and the environment. And most importantly it is able to adjust this policy ``on the go'', meaning it can both learn a new policy if the challenge or the environment changes and that it can be deployed immediately without any training and it will improve as it gains experience. In this context the agent's goal is always set by a reward function. Using reinforcement learning, the agent learns to maximize this function and thus, hopefully, to achieve the goal of its designers. 

Central to reinforcement learning are the policies and how best to evaluate them. In order to achieve its goal an agent needs to develop a policy that maximizes its reward, then as the agent encounters similar situations it simply follows the policy it has learned. To evaluate a policy it must be compared to the ideal version of itself. For any given problem and its reward function there exists at least one policy which maximizes the reward. At its core, reinforcement learning aims to enable the agent to continuously refine its current policy so that it approaches the optimal policy. 

Even more important than the concept of policies is the idea of exploration vs. exploitation. Since the agent initially knows nothing about its environment it must attempt  to learn about its environment through exploration. By trying different choices and receiving different rewards the agent can construct a policy that always makes the right choice. But in order to know what the right choice is the bandit must also make the wrong choice so that it can learn not to make it again. Trying different things is the ''exploration'' and using the knowledge gained from this to make the right choice is the ``exploitation''. An agent cannot simultaneously explore and exploit. This is dichotomy is at the core of reinforcement learning. The agent must always make the choice between exploring more to potentially discover an even better policy and eventually yield even better rewards, and using its current policy to increase its rewards. 

In this thesis two specific types of reinforcement learning are considered. First are gradient bandits. The term ``Bandits'' is a framework for solving problems in which an agent repeatedly returns to an unchanging situation in which there are several choices, each of which lead to unknown results. The analogy used by Sutton and Barto is of a room with several levers with unknown effects (the pulling of a certain lever may also be called the action). From the bandit's perspective pulling any of the levers yields a certain reward and it is the bandit's task to find a policy which yields the maximum reward from the pulling of certain levers. Applying this to the case of nextflow and the sizing of tasks one can consider the levers, or the choice, as the resource configuration. The bandit is asked repeatedly to allocate a certain amount of resources for a task (equivalent to pulling one of the levers) and must find the best policy for this (where the policy could be minimizing runtime or maximizing resource usage). Gradient bandits solve this problem of finding the best policy by using the gradient of the reward function to learn a preference for each of the levers. Using gradient ascent the bandits take small steps in the direction of a the ideal preference for each lever which would maximizes the reward. Mathematically speaking this is done through stochastic gradient ascent, for which the formula is : TODO use equation $H_{t+1} = H_t(a) + \alpha \frac{ \delta E[R_t] }  {\delta H_t(a)}$. This formula aims to increment the preference proportional to the increment's effect on performance. These preferences influence the probability of choosing a given action. Crucially, after choosing an action and receiving the reward the preferences for all of the actions are updated to try and follow the gradient of the reward. In practice however the expected reward for each action is not known- if it were known the problem would be trivial and the agent could be configured to always pick the maximizing action. Instead the expected reward function and its gradient must be approximated over time. This leads to the formula for updating the preferences proposed by Sutton and Barto. For a preference $H_{t+1}$ after taking action $A_t$ and receiving reward $R_t$ its preference is updated as follows TODO use begin equation: $H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \hat{R}) (1 - \pi_t(A_t))$ and $H_{t+1}(a_t) = H_t(a_t) - \alpha (R_t - \hat{R})\pi_t(a_t)$ for the preferences for each action $a_t != A_t$ , where $\hat{R}$ is the average of all the rewards so far. It can be proven that this formula eventually approximates the formula for gradient ascent.

TODO: states and Q or TD learning
